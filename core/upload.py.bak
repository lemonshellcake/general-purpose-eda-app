"""
Module for handling data upload functionality of the EDA app.
"""
from typing import Tuple, Dict, Optional, Any
import pandas as pd
import numpy as np
import streamlit as st
from pathlib import Path
import io

def validate_file_type(file: Any) -> bool:
    """
    Validate if the uploaded file has an acceptable extension.
    
    Args:
        file: Streamlit file uploader object
        
    Returns:
        bool: True if file is of acceptable type, False otherwise
    """
    acceptable_extensions = ['.csv', '.xlsx', '.xls']
    file_extension = Path(file.name).suffix.lower()
    return file_extension in acceptable_extensions

def validate_file_size(file: Any, max_size_gb: float = 5.0) -> bool:
    """
    Validate if the uploaded file does not exceed max size.
    
    Args:
        file: Streamlit file uploader object
        max_size_gb: Maximum allowed file size in gigabytes
        
    Returns:
        bool: True if file size is acceptable, False otherwise
    """
    bytes_content = file.getvalue()
    file_size_gb = len(bytes_content) / (1024**3)  # Convert to GB
    return file_size_gb <= max_size_gb

def validate_dataframe(df: pd.DataFrame) -> Tuple[bool, str]:
    """
    Validate if the dataframe has at least one numeric column.
    
    Args:
        df: Pandas dataframe to validate
        
    Returns:
        Tuple[bool, str]: (is_valid, message)
    """
    if df.empty:
        return False, "The uploaded file contains no data."
    
    numeric_columns = df.select_dtypes(include=['number']).columns
    if len(numeric_columns) == 0:
        return False, "The dataset must contain at least one numeric column."
    
    return True, "Dataset is valid."

def load_file(file: Any) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
    """
    Load a file into a pandas dataframe.
    
    Args:
        file: Streamlit file uploader object
        
    Returns:
        Tuple[pd.DataFrame, str]: (dataframe, error_message)
    """
    try:
        file_extension = Path(file.name).suffix.lower()
        
        if file_extension == '.csv':
            # Try to infer CSV delimiter
            df = pd.read_csv(file, sep=None, engine='python')
        elif file_extension in ['.xlsx', '.xls']:
            df = pd.read_excel(file)
        else:
            return None, f"Unsupported file extension: {file_extension}"
        
        return df, None
    except Exception as e:
        return None, f"Error loading file: {str(e)}"

def load_example_dataset(dataset_name: str) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
    """
    Load an example dataset from the data directory.
    
    Args:
        dataset_name: Name of the example dataset to load
        
    Returns:
        Tuple[Optional[pd.DataFrame], Optional[str]]: (dataframe, error_message)
    """
    try:
        from pathlib import Path
        import os
        
        # Get the path to the data directory
        app_dir = Path(__file__).parent.parent
        data_dir = os.path.join(app_dir, "data")
        
        # Map dataset names to file paths
        dataset_files = {
            "Student Performance (SAP-4000)": os.path.join(data_dir, "SAP-4000.csv"),
            # Add more datasets as they become available
        }
        
        if dataset_name not in dataset_files:
            return None, f"Dataset {dataset_name} not found"
            
        file_path = dataset_files[dataset_name]
        
        # Load the dataset based on file extension
        file_extension = Path(file_path).suffix.lower()
        if file_extension == '.csv':
            df = pd.read_csv(file_path, sep=None, engine='python')
        elif file_extension in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
        else:
            return None, f"Unsupported file extension: {file_extension}"
            
        return df, None
        
    except Exception as e:
        return None, f"Error loading example dataset: {str(e)}"


def upload_data() -> Tuple[Optional[pd.DataFrame], Dict[str, Any]]:
    """
    Handle file upload, validation, and initial loading.
    
    Returns:
        Tuple[pd.DataFrame, Dict[str, Any]]: (dataframe, session_state)
    """    session_state = {
        'upload_success': False,
        'df': None,
        'error_message': None,
        'filename': None
    }
    
    st.title("Data Upload")
    
    # Create tabs for different upload methods
    tabs = st.tabs(["Upload Your Data", "Use Example Dataset"])
    
    # Track which tab is active
    using_example = False
    df = None
    error = None
    
    # Tab 1: Upload your data
    with tabs[0]:
        uploaded_file = st.file_uploader("Choose a data file", type=['csv', 'xlsx', 'xls'])
        if uploaded_file is not None:
            # Validate file type
            if not validate_file_type(uploaded_file):
                session_state['error_message'] = f"Invalid file type. Please upload a .csv, .xlsx, or .xls file."
                st.error(session_state['error_message'])
                return None, session_state
            
            # Validate file size
            if not validate_file_size(uploaded_file):
                session_state['error_message'] = f"File size exceeds the 5GB limit."
                st.error(session_state['error_message'])
                return None, session_state
              # Load data
            df, error = load_file(uploaded_file)
            if uploaded_file.name:
                session_state['filename'] = uploaded_file.name
    
    # Tab 2: Use example dataset
    with tabs[1]:
        st.write("Choose an example dataset to explore the app features:")
        
        # Get available example datasets
        example_datasets = ["Student Performance (SAP-4000)"]
        
        selected_dataset = st.selectbox(
            "Select example dataset",
            options=example_datasets,
            index=0,
        )
        
        if st.button("Load Example Dataset"):
            using_example = True
            df, error = load_example_dataset(selected_dataset)
            if selected_dataset:
                session_state['filename'] = f"Example: {selected_dataset}"
    
    # Process the data regardless of source
    if df is not None:
        if error:
            session_state['error_message'] = error
            st.error(error)
            return None, session_state
        
        # Validate dataframe
        is_valid, message = validate_dataframe(df)
        if not is_valid:
            session_state['error_message'] = message
            st.error(message)
            return None, session_state
        
        # Update session state with successful upload
        session_state['upload_success'] = True
        session_state['df'] = df
        
        # Display success message and preview
        source_text = "Example dataset" if using_example else f"File '{session_state['filename']}'"
        st.success(f"{source_text} successfully loaded!")
        
        st.write("Data Preview:")
        st.dataframe(df.head())
        
        # Display basic info
        st.write("Basic Information:")
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"**Rows:** {df.shape[0]}")
            st.write(f"**Columns:** {df.shape[1]}")
        
        with col2:
            st.write(f"**Numeric columns:** {len(df.select_dtypes(include=['number']).columns)}")
            st.write(f"**Categorical columns:** {len(df.select_dtypes(exclude=['number']).columns)}")
        
        # Show detailed info in an expander
        with st.expander("Show detailed data information"):
            buffer = io.StringIO()
            df.info(buf=buffer)
            st.text(buffer.getvalue())
        
        return df, session_state
    
    return None, session_state
